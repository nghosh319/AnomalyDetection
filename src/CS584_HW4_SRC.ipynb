{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d52de9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step1:\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from scipy.fft import fft\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_data(mode):\n",
    "    data = []\n",
    "    labels = []\n",
    "    mode_directory = f\"./{mode}/\"\n",
    "    \n",
    "    # Get the list of file names and extract numbers from the file names\n",
    "    file_names = os.listdir(mode_directory)\n",
    "    file_numbers = [int(re.findall(r'\\d+', file_name)[0]) for file_name in file_names]\n",
    "    sorted_file_indices = np.argsort(file_numbers)\n",
    "    sorted_file_names = [file_names[i] for i in sorted_file_indices]\n",
    "\n",
    "#     print(sorted_file_names) \n",
    "        \n",
    "    for file_name in sorted_file_names:\n",
    "        if file_name.endswith('.txt'):\n",
    "            file_path = os.path.join(mode_directory, file_name)\n",
    "            with open(file_path, 'r') as file:\n",
    "                signal_data = [float(value) for value in file.read().split()]\n",
    "\n",
    "                # Check if the file is empty before adding it to the data\n",
    "                if len(signal_data) > 0:\n",
    "                    data.append(signal_data)\n",
    "                    labels.append(0 if mode == 'base/ModeM' else 1)  # 0 for ModeM, 1 for normal modes\n",
    "\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "def extract_features(data):\n",
    "    # Apply Fast Fourier Transform (FFT) on each signal\n",
    "    transformed_data = []\n",
    "    for signal in data:\n",
    "        fft_result = np.abs(fft(signal))\n",
    "        transformed_data.append(fft_result)\n",
    "\n",
    "    return np.array(transformed_data)\n",
    "\n",
    "# Load data from all modes (A, B, C, D, and M)\n",
    "mode_a_data, mode_a_labels = load_data('base/ModeA')\n",
    "mode_b_data, mode_b_labels = load_data('base/ModeB')\n",
    "mode_c_data, mode_c_labels = load_data('base/ModeC')\n",
    "mode_d_data, mode_d_labels = load_data('base/ModeD')\n",
    "mode_m_data, mode_m_labels = load_data('base/ModeM')\n",
    "\n",
    "# Create random samples for training and testing by using train_test_split with 0.1\n",
    "validation_ratio = 0.1 \n",
    "mode_a_data_train, mode_a_data_test, mode_a_labels_train, mode_a_labels_test = train_test_split(\n",
    "    mode_a_data, mode_a_labels, test_size=validation_ratio, random_state=42)\n",
    "\n",
    "mode_b_data_train, mode_b_data_test, mode_b_labels_train, mode_b_labels_test = train_test_split(\n",
    "    mode_b_data, mode_b_labels, test_size=validation_ratio, random_state=42)\n",
    "\n",
    "mode_c_data_train, mode_c_data_test, mode_c_labels_train, mode_c_labels_test = train_test_split(\n",
    "    mode_c_data, mode_c_labels, test_size=validation_ratio, random_state=42)\n",
    "\n",
    "mode_d_data_train, mode_d_data_test, mode_d_labels_train, mode_d_labels_test = train_test_split(\n",
    "    mode_d_data, mode_d_labels, test_size=validation_ratio, random_state=42)\n",
    "\n",
    "mode_m_data_train, mode_m_data_test, mode_m_labels_train, mode_m_labels_test = train_test_split(\n",
    "    mode_m_data, mode_m_labels, test_size=validation_ratio, random_state=42)\n",
    "\n",
    "\n",
    "# Combine normal mode data and labels into one dataset\n",
    "normal_mode_data_train = np.concatenate((mode_a_data_train, mode_b_data_train, mode_c_data_train, mode_d_data_train), axis=0)\n",
    "normal_mode_labels_train = np.concatenate((mode_a_labels_train, mode_b_labels_train, mode_c_labels_train, mode_d_labels_train), axis=0)\n",
    "\n",
    "\n",
    "# Extract features using FFT\n",
    "baseline_data = extract_features(normal_mode_data_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "120988dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(359, 20000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4123bb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "12/12 [==============================] - 2s 70ms/step - loss: 49488144.0000\n",
      "Epoch 2/50\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 49487400.0000\n",
      "Epoch 3/50\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 49487232.0000\n",
      "Epoch 4/50\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 49487140.0000\n",
      "Epoch 5/50\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 49487072.0000\n",
      "Epoch 6/50\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 49487004.0000\n",
      "Epoch 7/50\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 49486960.0000\n",
      "Epoch 8/50\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 49486912.0000\n",
      "Epoch 9/50\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 49486880.0000\n",
      "Epoch 10/50\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 49486856.0000\n",
      "Epoch 11/50\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 49486824.0000\n",
      "Epoch 12/50\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 49486800.0000\n",
      "Epoch 13/50\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 49486788.0000\n",
      "Epoch 14/50\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 49486748.0000\n",
      "Epoch 15/50\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 49486696.0000\n",
      "Epoch 16/50\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 49486652.0000\n",
      "Epoch 17/50\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 49486612.0000\n",
      "Epoch 18/50\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 49486596.0000\n",
      "Epoch 19/50\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 49486576.0000\n",
      "Epoch 20/50\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 49486564.0000\n",
      "Epoch 21/50\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 49486556.0000\n",
      "Epoch 22/50\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 49486556.0000\n",
      "Epoch 23/50\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 49486556.0000\n",
      "Epoch 24/50\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 49486532.0000\n",
      "Epoch 25/50\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 49486524.0000\n",
      "Epoch 26/50\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 49486520.0000\n",
      "Epoch 27/50\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 49486508.0000\n",
      "Epoch 28/50\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 49486476.0000\n",
      "Epoch 29/50\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 49486464.0000\n",
      "Epoch 30/50\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 49486452.0000\n",
      "Epoch 31/50\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 49486436.0000\n",
      "Epoch 32/50\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 49486400.0000\n",
      "Epoch 33/50\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 49486364.0000\n",
      "Epoch 34/50\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 49486348.0000\n",
      "Epoch 35/50\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 49486336.0000\n",
      "Epoch 36/50\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 49486332.0000\n",
      "Epoch 37/50\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 49486316.0000\n",
      "Epoch 38/50\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 49486296.0000\n",
      "Epoch 39/50\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 49486280.0000\n",
      "Epoch 40/50\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 49486264.0000\n",
      "Epoch 41/50\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 49486248.0000\n",
      "Epoch 42/50\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 49486240.0000\n",
      "Epoch 43/50\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 49486228.0000\n",
      "Epoch 44/50\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 49486228.0000\n",
      "Epoch 45/50\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 49486228.0000\n",
      "Epoch 46/50\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 49486228.0000\n",
      "Epoch 47/50\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 49486224.0000\n",
      "Epoch 48/50\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 49486224.0000\n",
      "Epoch 49/50\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 49486216.0000\n",
      "Epoch 50/50\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 49486224.0000\n",
      "12/12 [==============================] - 1s 11ms/step\n"
     ]
    }
   ],
   "source": [
    "# Step2:\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Function to create and train the standard Autoencoder (AUE)\n",
    "# Standard Autoencoder:\n",
    "def create_autoencoder(input_dim):\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoded = Dense(128, activation='relu')(input_layer)\n",
    "    decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    return autoencoder\n",
    "\n",
    "# Sparse Autoencoder:\n",
    "from keras import regularizers\n",
    "\n",
    "def create_sparse_autoencoder(input_dim, sparsity_factor=0.1):\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoded = Dense(128, activation='relu', activity_regularizer=regularizers.l1(sparsity_factor))(input_layer)\n",
    "    decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    return autoencoder\n",
    "\n",
    "\n",
    "# Create and train the AUE using the baseline data\n",
    "input_dim = baseline_data.shape[1]\n",
    "autoencoder = create_autoencoder(input_dim)\n",
    "autoencoder.fit(baseline_data, baseline_data, epochs=50, batch_size=32, shuffle=True)\n",
    "\n",
    "# Function to compute the reconstruction error for each baseline data point\n",
    "def compute_reconstruction_errors(data, autoencoder):\n",
    "    reconstructed_data = autoencoder.predict(data)\n",
    "    errors = np.mean((data - reconstructed_data) ** 2, axis=1)\n",
    "    return errors\n",
    "\n",
    "# Compute the reconstruction errors for the baseline data\n",
    "baseline_errors = compute_reconstruction_errors(baseline_data, autoencoder)\n",
    "\n",
    "# Sort the errors in ascending order to get the strangeness training list\n",
    "strangeness_training_list = sorted(baseline_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76a4aa40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32487489.433941748,\n",
       " 34954822.47939434,\n",
       " 35373177.00636929,\n",
       " 36062298.46131416,\n",
       " 36398516.079635724,\n",
       " 36421246.72735629,\n",
       " 37111511.85174423,\n",
       " 37572521.95020863,\n",
       " 37757022.678125314,\n",
       " 37942187.595069535,\n",
       " 37981382.2094477,\n",
       " 38011967.97947988,\n",
       " 38097885.71071648,\n",
       " 38173329.517251864,\n",
       " 38253546.52345376,\n",
       " 38553443.23279424,\n",
       " 38725026.060353994,\n",
       " 38760227.21303732,\n",
       " 38879549.440432124,\n",
       " 38884581.62048061,\n",
       " 39029589.45943749,\n",
       " 39136480.673666194,\n",
       " 39512396.80635826,\n",
       " 39843505.85360439,\n",
       " 39881047.69433657,\n",
       " 40158314.82839735,\n",
       " 40163190.16098584,\n",
       " 40272271.31767195,\n",
       " 40482750.635243416,\n",
       " 40547361.666336305,\n",
       " 40791367.87699693,\n",
       " 40910419.479755245,\n",
       " 41319078.57384192,\n",
       " 41353056.32492645,\n",
       " 41363484.93158914,\n",
       " 41775962.213933006,\n",
       " 41874902.19310641,\n",
       " 42026355.92233774,\n",
       " 42120001.88429694,\n",
       " 42135630.04760939,\n",
       " 42136807.28887127,\n",
       " 42189588.880703226,\n",
       " 42190766.75364763,\n",
       " 42319363.78288391,\n",
       " 42486937.073319085,\n",
       " 42600066.10237312,\n",
       " 42601607.429063134,\n",
       " 42771733.30478636,\n",
       " 42790830.56142981,\n",
       " 42870758.093551114,\n",
       " 42871531.39518124,\n",
       " 42871936.53197522,\n",
       " 43030443.20101492,\n",
       " 43217331.931163855,\n",
       " 43294638.02539775,\n",
       " 43369001.9834365,\n",
       " 43460622.29418636,\n",
       " 43540519.937787235,\n",
       " 43620801.78660171,\n",
       " 43627274.03135357,\n",
       " 43654609.752148144,\n",
       " 43736189.45448383,\n",
       " 43796627.05013573,\n",
       " 43985606.93057037,\n",
       " 43990238.6409143,\n",
       " 43994766.45431998,\n",
       " 44041742.02913244,\n",
       " 44086994.705614194,\n",
       " 44098706.16012483,\n",
       " 44245698.03345751,\n",
       " 44249516.69533768,\n",
       " 44250697.19523583,\n",
       " 44260668.988553286,\n",
       " 44323905.196505934,\n",
       " 44422667.49229263,\n",
       " 44469115.333209224,\n",
       " 44469303.6209944,\n",
       " 44476162.416447334,\n",
       " 44504331.39817433,\n",
       " 44615429.37770104,\n",
       " 44732897.72355798,\n",
       " 44756052.01881622,\n",
       " 44834508.95741406,\n",
       " 44951942.06736911,\n",
       " 45032980.395766355,\n",
       " 45136810.606607474,\n",
       " 45144714.89011614,\n",
       " 45168734.41047666,\n",
       " 45223655.236652635,\n",
       " 45257305.06374964,\n",
       " 45278046.152206644,\n",
       " 45415038.37621269,\n",
       " 45447943.38244529,\n",
       " 45461978.134925336,\n",
       " 45462297.59657414,\n",
       " 45507515.71723731,\n",
       " 45597090.69036544,\n",
       " 45799111.81964015,\n",
       " 45850493.56532063,\n",
       " 45852124.13922637,\n",
       " 45882651.67666657,\n",
       " 46143248.13102191,\n",
       " 46150919.90692201,\n",
       " 46249140.93763558,\n",
       " 46264483.56211809,\n",
       " 46308258.738049455,\n",
       " 46388276.806020364,\n",
       " 46459675.45794857,\n",
       " 46506210.02252763,\n",
       " 46584770.85492682,\n",
       " 46598680.19919885,\n",
       " 46624808.44335181,\n",
       " 46652676.19745495,\n",
       " 46697186.46985572,\n",
       " 46701506.419867136,\n",
       " 46755764.156146854,\n",
       " 46864380.978079796,\n",
       " 46955457.62496864,\n",
       " 47073319.70905683,\n",
       " 47105011.99452482,\n",
       " 47228871.04953666,\n",
       " 47241108.91680866,\n",
       " 47242245.870498665,\n",
       " 47264202.81100805,\n",
       " 47275643.09716401,\n",
       " 47287353.45792623,\n",
       " 47312547.60384955,\n",
       " 47345412.48378559,\n",
       " 47353037.3937027,\n",
       " 47463129.66043383,\n",
       " 47467172.764596075,\n",
       " 47477494.117336914,\n",
       " 47533162.919884644,\n",
       " 47573784.27038414,\n",
       " 47588375.203851886,\n",
       " 47602950.274214804,\n",
       " 47608900.95328463,\n",
       " 47687049.73419208,\n",
       " 47801490.0525058,\n",
       " 47824823.63768893,\n",
       " 47832757.58528783,\n",
       " 47864731.40054324,\n",
       " 47881884.02679697,\n",
       " 47888656.47459432,\n",
       " 47930057.29397413,\n",
       " 47943488.61696377,\n",
       " 48394224.67311054,\n",
       " 48396100.049182184,\n",
       " 48447705.605071925,\n",
       " 48543744.77110207,\n",
       " 48584147.365097195,\n",
       " 48679973.428806834,\n",
       " 48684675.713563554,\n",
       " 48791783.82019625,\n",
       " 48830930.82355335,\n",
       " 48972544.40085783,\n",
       " 48986659.65201444,\n",
       " 49011347.812673144,\n",
       " 49051744.901089475,\n",
       " 49059402.705017395,\n",
       " 49108978.295401625,\n",
       " 49128585.651790366,\n",
       " 49171492.960915655,\n",
       " 49195864.38261879,\n",
       " 49198435.706951536,\n",
       " 49202988.12300702,\n",
       " 49205805.766696215,\n",
       " 49248240.76025024,\n",
       " 49248714.68049404,\n",
       " 49289309.68646878,\n",
       " 49349701.64213961,\n",
       " 49382217.834827125,\n",
       " 49428758.378220506,\n",
       " 49459620.59568768,\n",
       " 49476392.414139576,\n",
       " 49481194.52546723,\n",
       " 49492078.67606776,\n",
       " 49497621.04997466,\n",
       " 49567463.39372997,\n",
       " 49631614.41658537,\n",
       " 49643636.92890537,\n",
       " 49670892.52025278,\n",
       " 49713199.16600402,\n",
       " 49728853.146212585,\n",
       " 49901920.76652215,\n",
       " 49977999.847596094,\n",
       " 50041332.99192403,\n",
       " 50063803.53055915,\n",
       " 50070231.286025874,\n",
       " 50108097.22388675,\n",
       " 50146616.840881675,\n",
       " 50174107.15445066,\n",
       " 50184196.45417008,\n",
       " 50237312.46554658,\n",
       " 50243816.72910257,\n",
       " 50248752.357995436,\n",
       " 50356840.01455173,\n",
       " 50386533.58210936,\n",
       " 50420388.61167501,\n",
       " 50516131.757867746,\n",
       " 50527372.76867073,\n",
       " 50657371.20492049,\n",
       " 50699732.37220189,\n",
       " 50909565.25252241,\n",
       " 50918276.6961405,\n",
       " 50945559.54483827,\n",
       " 50991682.16856144,\n",
       " 51029357.53465034,\n",
       " 51050878.08291619,\n",
       " 51075347.20834958,\n",
       " 51082013.78870966,\n",
       " 51089192.10284906,\n",
       " 51117755.12607728,\n",
       " 51119746.86861946,\n",
       " 51151957.02986879,\n",
       " 51198435.178961664,\n",
       " 51227774.970727965,\n",
       " 51274845.95246142,\n",
       " 51357632.513708636,\n",
       " 51378241.86236978,\n",
       " 51405609.01031493,\n",
       " 51447901.10092518,\n",
       " 51454486.78073516,\n",
       " 51471318.80354524,\n",
       " 51481161.18081109,\n",
       " 51504773.423383296,\n",
       " 51539455.424218066,\n",
       " 51553253.258552514,\n",
       " 51631317.6983371,\n",
       " 51809862.43620439,\n",
       " 51821563.03775354,\n",
       " 51853121.02659286,\n",
       " 51939609.16792235,\n",
       " 51951863.74923274,\n",
       " 51995543.95472914,\n",
       " 52099216.695221014,\n",
       " 52204334.669238865,\n",
       " 52263017.070772,\n",
       " 52282164.45950217,\n",
       " 52291678.277530156,\n",
       " 52363491.73400072,\n",
       " 52442781.83469226,\n",
       " 52455122.09221579,\n",
       " 52485158.92716313,\n",
       " 52583302.451545596,\n",
       " 52626445.45721217,\n",
       " 52684288.98126835,\n",
       " 52715765.00827177,\n",
       " 52735843.27037094,\n",
       " 52749982.852007955,\n",
       " 52812166.83266005,\n",
       " 52822933.85016399,\n",
       " 52838126.035324514,\n",
       " 52908318.43120958,\n",
       " 52930242.59356128,\n",
       " 52978604.578983456,\n",
       " 53053050.8082197,\n",
       " 53079929.46339648,\n",
       " 53088259.997754075,\n",
       " 53152854.94473029,\n",
       " 53184742.07242779,\n",
       " 53192594.532529995,\n",
       " 53267120.980869375,\n",
       " 53437066.00260293,\n",
       " 53472201.903526254,\n",
       " 53481001.256650746,\n",
       " 53584677.963706724,\n",
       " 53585238.20568714,\n",
       " 53590095.52443716,\n",
       " 53617000.144143425,\n",
       " 53640073.65990229,\n",
       " 53711763.81272133,\n",
       " 53712995.82548345,\n",
       " 53721672.08826987,\n",
       " 53733346.43560828,\n",
       " 53842377.33183949,\n",
       " 53916610.366696246,\n",
       " 53957565.623619206,\n",
       " 54005241.06781681,\n",
       " 54018103.346681036,\n",
       " 54091150.742863566,\n",
       " 54132736.186239935,\n",
       " 54156353.75877112,\n",
       " 54200455.77731828,\n",
       " 54265850.93046291,\n",
       " 54377117.592441335,\n",
       " 54377751.13905993,\n",
       " 54402609.06176442,\n",
       " 54461536.08199872,\n",
       " 54516361.19810588,\n",
       " 54802546.34788199,\n",
       " 54803911.61617692,\n",
       " 54803984.499709636,\n",
       " 54804983.32684026,\n",
       " 54844464.46160698,\n",
       " 54850599.85506574,\n",
       " 54905968.84427706,\n",
       " 54944897.89749883,\n",
       " 55067972.511385,\n",
       " 55096275.26284326,\n",
       " 55114447.92912679,\n",
       " 55147996.02542716,\n",
       " 55193594.15557357,\n",
       " 55437738.758326866,\n",
       " 55568058.06592633,\n",
       " 55965383.38724667,\n",
       " 56088291.07428749,\n",
       " 56211449.84734633,\n",
       " 56368245.00408574,\n",
       " 56392090.489308804,\n",
       " 56443976.19053292,\n",
       " 56553095.66829725,\n",
       " 56561326.60099068,\n",
       " 56576040.86387751,\n",
       " 56615123.6618297,\n",
       " 56615828.6137526,\n",
       " 56673406.132630944,\n",
       " 56776648.25444931,\n",
       " 57058947.78928037,\n",
       " 57076434.10879862,\n",
       " 57081015.63285757,\n",
       " 57083220.79891455,\n",
       " 57089756.068789124,\n",
       " 57152553.46946858,\n",
       " 57155399.07650954,\n",
       " 57217629.53798694,\n",
       " 57240623.60812677,\n",
       " 57378072.49076075,\n",
       " 57449023.35409221,\n",
       " 57465760.3636317,\n",
       " 57559870.522197165,\n",
       " 57563927.41376289,\n",
       " 57630421.76367997,\n",
       " 57786239.64025034,\n",
       " 57932738.349161975,\n",
       " 57974482.49043132,\n",
       " 58137633.597461954,\n",
       " 58189174.686506115,\n",
       " 58316363.4205119,\n",
       " 58388609.7600799,\n",
       " 58420197.742712475,\n",
       " 58590370.43865759,\n",
       " 58849120.088537656,\n",
       " 59285700.630354635,\n",
       " 59519410.388651475,\n",
       " 60198281.83082923,\n",
       " 60261952.79841123,\n",
       " 60277465.63548524,\n",
       " 60720135.05675281,\n",
       " 61174059.46731286,\n",
       " 61325249.275281094,\n",
       " 61419842.400667615,\n",
       " 61544794.98885592,\n",
       " 62196284.84910494,\n",
       " 62442698.7617051,\n",
       " 62592607.4541151,\n",
       " 64687852.8246241,\n",
       " 65016581.63821237,\n",
       " 66842979.75488962]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strangeness_training_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81230ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step3\n",
    "\n",
    "# Combine test datasets\n",
    "test_data = np.concatenate((mode_a_data_test, mode_b_data_test, mode_c_data_test, mode_d_data_test, mode_m_data_test), axis=0)\n",
    "\n",
    "# Extract features using FFT\n",
    "test_data_fft = extract_features(test_data)\n",
    "\n",
    "# Combine test labels\n",
    "test_labels = np.concatenate((mode_a_labels_test, mode_b_labels_test, mode_c_labels_test, mode_d_labels_test, mode_m_labels_test), axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8e2aba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mode_a_labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3abb413e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 11ms/step\n"
     ]
    }
   ],
   "source": [
    "# step4\n",
    "\n",
    "# Use the Autoencoder to predict the reconstructed data for the test set\n",
    "test_data_reconstructed = autoencoder.predict(test_data_fft)\n",
    "\n",
    "# Calculate the reconstruction error for each data point\n",
    "reconstruction_errors_test = np.mean((test_data_fft - test_data_reconstructed)**2, axis=1)\n",
    "\n",
    "# Form a list of strangeness values for the test set\n",
    "strangeness_values_test = reconstruction_errors_test.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1094d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step5\n",
    "\n",
    "test_p_values = []\n",
    "N = len(strangeness_training_list)\n",
    "\n",
    "for test_strangeness in strangeness_values_test:\n",
    "    # Find the index of the test strangeness in the sorted training list\n",
    "    index = np.searchsorted(strangeness_training_list, test_strangeness)\n",
    "    \n",
    "    # Calculate the number of measures in the training list that are higher or equal to the test strangeness\n",
    "    b = N - index\n",
    "    \n",
    "    # Calculate the p-value\n",
    "    p_value = (b + 1) / (N + 1)\n",
    "    \n",
    "    # Append the p-value to the list of test p-values\n",
    "    test_p_values.append(p_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d7d4210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7333333333333333,\n",
       " 0.7583333333333333,\n",
       " 0.9305555555555556,\n",
       " 0.4361111111111111,\n",
       " 0.7333333333333333,\n",
       " 0.5638888888888889,\n",
       " 0.825,\n",
       " 0.7055555555555556,\n",
       " 0.55,\n",
       " 0.43333333333333335,\n",
       " 0.16111111111111112,\n",
       " 0.7777777777777778,\n",
       " 0.34444444444444444,\n",
       " 0.11388888888888889,\n",
       " 0.5944444444444444,\n",
       " 0.8416666666666667,\n",
       " 0.4638888888888889,\n",
       " 0.5222222222222223,\n",
       " 0.5944444444444444,\n",
       " 0.825,\n",
       " 0.15833333333333333,\n",
       " 0.7194444444444444,\n",
       " 0.34444444444444444,\n",
       " 0.45555555555555555,\n",
       " 0.4888888888888889,\n",
       " 0.5944444444444444,\n",
       " 0.39444444444444443,\n",
       " 0.9222222222222223,\n",
       " 0.7305555555555555,\n",
       " 0.7472222222222222,\n",
       " 0.3416666666666667,\n",
       " 0.09166666666666666,\n",
       " 0.22777777777777777,\n",
       " 0.1361111111111111,\n",
       " 0.325,\n",
       " 0.575,\n",
       " 0.04722222222222222,\n",
       " 0.3416666666666667,\n",
       " 0.28888888888888886,\n",
       " 0.175,\n",
       " 0.002777777777777778,\n",
       " 0.002777777777777778,\n",
       " 0.005555555555555556,\n",
       " 0.002777777777777778,\n",
       " 0.002777777777777778,\n",
       " 0.002777777777777778,\n",
       " 0.005555555555555556,\n",
       " 0.002777777777777778,\n",
       " 0.002777777777777778,\n",
       " 0.002777777777777778]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_p_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c88e9699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 1.0\n",
      "True Positives: 10\n",
      "False Positives: 1\n"
     ]
    }
   ],
   "source": [
    "# Step6\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Compute the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(test_labels, test_p_values)\n",
    "\n",
    "# Compute the AUC\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Print the AUC\n",
    "print(\"AUC:\", roc_auc)\n",
    "\n",
    "# Convert test_p_values to a numpy array\n",
    "test_p_values = np.array(test_p_values)\n",
    "\n",
    "# selecting a confidence level of 0.95\n",
    "confidence_level = 0.95\n",
    "\n",
    "# Compute the threshold for strangeness based on the confidence level\n",
    "\n",
    "threshold = 1 - confidence_level\n",
    "\n",
    "# Compute TP and FP\n",
    "true_positives = sum((test_p_values < threshold) & (test_labels == 0))\n",
    "false_positives = sum((test_p_values < threshold) & (test_labels == 1))\n",
    "\n",
    "# Print TP and FP\n",
    "print(\"True Positives:\", true_positives)\n",
    "print(\"False Positives:\", false_positives)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07963857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 11ms/step\n"
     ]
    }
   ],
   "source": [
    "# Step8\n",
    "\n",
    "# Load data from all modes (A, B, C, D, and M)\n",
    "test_data_min2,test_data_label = load_data('Test\\TestWT')\n",
    "# Extract features using FFT\n",
    "test_data_min2_fft = extract_features(test_data_min2)\n",
    "\n",
    "# Use the Autoencoder to predict the reconstructed data for the test set\n",
    "test_data_reconstructed_min2 = autoencoder.predict(test_data_min2_fft)\n",
    "\n",
    "# Calculate the reconstruction error for each data point\n",
    "reconstruction_errors_test_min2 = np.mean((test_data_min2_fft - test_data_reconstructed_min2)**2, axis=1)\n",
    "\n",
    "# Form a list of strangeness values for the test set\n",
    "strangeness_values_test_min2 = reconstruction_errors_test_min2.tolist()\n",
    "\n",
    "# calculate p value\n",
    "test_min2_p_values = []\n",
    "N = len(strangeness_training_list)\n",
    "\n",
    "for test_strangeness_min2 in strangeness_values_test_min2:\n",
    "    # Find the index of the test strangeness in the sorted training list\n",
    "    index = np.searchsorted(strangeness_training_list, test_strangeness_min2)\n",
    "    \n",
    "    # Calculate the number of measures in the training list that are higher or equal to the test strangeness\n",
    "    b = N - index\n",
    "    \n",
    "    # Calculate the p-value\n",
    "    p_value_min2 = (b + 1) / (N + 1)\n",
    "    \n",
    "    # Append the p-value to the list of test p-values\n",
    "    test_min2_p_values.append(p_value_min2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac5f02a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the p-values to a text file\n",
    "np.savetxt('p_values_min2.txt', test_min2_p_values, fmt='%.6f')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
